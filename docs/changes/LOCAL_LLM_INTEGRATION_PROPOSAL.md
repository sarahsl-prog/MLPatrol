# Local LLM Integration Proposal

**Date:** 2025-11-14
**Type:** Feature Enhancement
**Status:** üìã Proposal

---

## Executive Summary

This document proposes integrating local LLM support into MLPatrol, allowing users to run the application without cloud API dependencies (Anthropic/OpenAI). After comprehensive research and analysis, **Ollama is recommended as the primary local LLM solution** due to its superior ease of use, LangChain integration, and suitability for MLPatrol's use case.

---

## Current Architecture Analysis

### Existing LLM Integration

**Location:** [src/agent/reasoning_chain.py:664-741](../../src/agent/reasoning_chain.py#L664-L741)

```python
def create_mlpatrol_agent(
    api_key: Optional[str] = None,
    model: str = "claude-sonnet-4-0",
    verbose: bool = True,
    **kwargs,
) -> MLPatrolAgent:
    # Initialize LLM based on model choice
    if "claude" in model.lower():
        from langchain_anthropic import ChatAnthropic
        llm = ChatAnthropic(...)
    elif "gpt" in model.lower():
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(...)
```

### Current Dependencies

```
langchain>=1.0.0
langchain-core>=1.0.0
langchain-openai>=1.0.0
langchain-anthropic>=1.0.0
langchain-community>=1.0.0
```

### Agent Requirements

- **Tools:** MLPatrol uses LangGraph with tool calling (CVE search, dataset analysis, etc.)
- **Reasoning:** Multi-step reasoning with ReAct pattern
- **Temperature:** Low (0.1) for factual security analysis
- **Max Tokens:** 4096
- **Use Case:** Security analysis, vulnerability detection, code generation

---

## Local LLM Solutions Research

### Option 1: Ollama (RECOMMENDED ‚≠ê)

#### Overview
- **Ease of Use:** üü¢ Excellent - "Docker for LLMs"
- **Performance:** üü° Good - Optimized for single-user/development
- **LangChain Integration:** üü¢ Excellent - First-class support
- **Setup Complexity:** üü¢ Very Low
- **Hardware:** Consumer GPUs, Apple Silicon (M1/M2)

#### Pros
‚úÖ **Extremely easy to install and use**
- One-line installation: `curl https://ollama.ai/install.sh | sh`
- Simple model management: `ollama pull llama3.1`
- No complex configuration needed

‚úÖ **Excellent LangChain integration**
- Dedicated `langchain-ollama` package
- Drop-in replacement with `ChatOllama` class
- Full tool calling support
- JSON mode support

‚úÖ **Great for MLPatrol's use case**
- Perfect for single-user security analysis
- Low latency for interactive queries
- Runs well on local hardware (even laptops)
- First-class Apple Silicon support

‚úÖ **Large model ecosystem**
- 30+ supported models (Llama 3, Mistral, DeepSeek, Phi-3)
- Easy model switching
- Optimized model variants (8B, 70B, etc.)

#### Cons
‚ùå Lower throughput under heavy concurrent load
‚ùå Not optimized for production-scale serving
‚ùå Limited to ~22 req/sec with high concurrency

#### Recommended Models for Security Analysis

| Model | Size | Best For | Context | Tool Calling |
|-------|------|----------|---------|--------------|
| **llama3.1:8b** | 8B | Fast, general security queries | 128K | ‚úÖ Yes |
| **llama3.1:70b** | 70B | Complex analysis, high accuracy | 128K | ‚úÖ Yes |
| **mistral-small:3.1** | 22B | Balanced performance/accuracy | 32K | ‚úÖ Yes |
| **qwen2.5:14b** | 14B | Code analysis, reasoning | 128K | ‚úÖ Yes |
| **deepseek-r1:7b** | 7B | Reasoning-optimized | 64K | ‚úÖ Yes |

#### Installation & Setup

```bash
# Install Ollama (Linux/Mac)
curl https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download

# Pull a model
ollama pull llama3.1:8b

# Test
ollama run llama3.1:8b
```

#### Code Integration

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1:8b",
    temperature=0.1,
    base_url="http://localhost:11434",  # Default Ollama server
)
```

---

### Option 2: vLLM

#### Overview
- **Ease of Use:** üî¥ Moderate - Requires more setup
- **Performance:** üü¢ Excellent - 3.23x faster than Ollama under load
- **LangChain Integration:** üü° Good - Two methods available
- **Setup Complexity:** üü° Moderate
- **Hardware:** Datacenter GPUs (A100, H100) preferred

#### Pros
‚úÖ **Exceptional performance**
- Up to 3.23x faster throughput than Ollama
- Dynamic batching for concurrent requests
- Efficient GPU memory utilization
- Production-grade serving

‚úÖ **Scalability**
- Multi-GPU support (tensor parallelism)
- High concurrent request handling
- Built for enterprise workloads

‚úÖ **LangChain integration options**
- Direct: `VLLM` class from `langchain_community`
- Server: OpenAI-compatible API with `ChatOpenAI`

#### Cons
‚ùå More complex setup (server + client architecture)
‚ùå Designed for datacenter GPUs (may be overkill for MLPatrol)
‚ùå Requires more technical expertise
‚ùå Higher resource requirements

#### Use Case Fit
- **Not ideal for MLPatrol** - Designed for production-scale serving
- Better suited for enterprise deployments with heavy load
- Overkill for single-user security analysis tool

---

### Option 3: LM Studio

#### Overview
- **Ease of Use:** üü¢ Excellent - Best GUI
- **Performance:** üü° Good
- **LangChain Integration:** üü¢ Good - OpenAI-compatible API
- **Setup Complexity:** üü¢ Very Low
- **Hardware:** Consumer GPUs, CPU inference

#### Pros
‚úÖ Best graphical interface
‚úÖ Easy model management
‚úÖ Built-in chat interface
‚úÖ OpenAI-compatible API server
‚úÖ Windows/Mac/Linux support

#### Cons
‚ùå GUI-focused (less suitable for server deployments)
‚ùå No native LangChain package (uses OpenAI compat layer)
‚ùå May have licensing considerations for commercial use

#### Use Case Fit
- Good for development/testing with GUI
- Less ideal for production deployment
- Better as a developer tool than production backend

---

### Option 4: LocalAI

#### Overview
- **Ease of Use:** üü° Moderate
- **Performance:** üü° Good
- **LangChain Integration:** üü¢ Good - OpenAI-compatible
- **Setup Complexity:** üü° Moderate

#### Pros
‚úÖ OpenAI API drop-in replacement
‚úÖ Enterprise-grade features
‚úÖ Multi-modal support
‚úÖ Data sovereignty focus

#### Cons
‚ùå More complex than Ollama
‚ùå Smaller community than Ollama
‚ùå Requires Docker/server setup

---

## Recommendation: Ollama

### Why Ollama is Best for MLPatrol

1. **Ease of Use Matches MLPatrol's Goals**
   - MLPatrol is a security analysis tool, not a high-throughput API service
   - Users want to analyze datasets and CVEs interactively, not serve thousands of requests
   - Simple setup means lower barrier to entry for security practitioners

2. **Perfect Performance Profile**
   - Single-user interactive queries: Ollama excels here
   - Low latency responses: Critical for good UX
   - Security analysis doesn't need vLLM's concurrent request optimization

3. **Excellent LangChain Integration**
   - `langchain-ollama` is officially maintained
   - Drop-in replacement for existing code
   - Full tool calling support (critical for MLPatrol's agent architecture)

4. **Hardware Accessibility**
   - Runs on consumer GPUs and Apple Silicon
   - Most MLPatrol users likely have laptops, not datacenter GPUs
   - Lower system requirements = wider user base

5. **Open Source & Privacy**
   - Fully local execution (no data leaves the machine)
   - Perfect for security-sensitive workloads
   - No API costs or rate limits

---

## Implementation Plan

### Phase 1: Add Ollama Support (Minimal Changes)

#### 1.1 Update Dependencies

**File:** `requirements.txt`

```diff
# LLM Provider SDKs
openai>=2.0.0  # For OpenAI models
anthropic>=0.25.0  # For Claude models
+langchain-ollama>=0.2.0  # For Ollama local models
```

#### 1.2 Update Agent Factory Function

**File:** `src/agent/reasoning_chain.py`

**Location:** Lines 664-741

```python
def create_mlpatrol_agent(
    api_key: Optional[str] = None,
    model: str = "claude-sonnet-4-0",
    base_url: Optional[str] = None,  # NEW: For Ollama/LocalAI
    verbose: bool = True,
    **kwargs,
) -> MLPatrolAgent:
    """Factory function to create a configured MLPatrol agent.

    Args:
        api_key: API key for cloud LLM providers (Anthropic/OpenAI)
        model: Model to use. Options:
            Cloud:
            - "claude-sonnet-4-0" (default, recommended)
            - "gpt-4"

            Local (Ollama):
            - "ollama/llama3.1:8b" (fast, recommended for local)
            - "ollama/llama3.1:70b" (high accuracy)
            - "ollama/mistral-small:3.1" (balanced)
            - "ollama/qwen2.5:14b" (code analysis)

            Local (vLLM):
            - "vllm/model-name" (production serving)

        base_url: Base URL for local LLM servers
            - Ollama: "http://localhost:11434" (default)
            - vLLM: "http://localhost:8000"
            - LM Studio: "http://localhost:1234/v1"

        verbose: Whether to enable verbose logging
        **kwargs: Additional arguments passed to MLPatrolAgent

    Returns:
        Configured MLPatrolAgent instance
    """
    try:
        # Determine LLM type from model string
        model_lower = model.lower()

        # Cloud LLMs
        if "claude" in model_lower:
            from langchain_anthropic import ChatAnthropic
            llm = ChatAnthropic(
                model=model,
                anthropic_api_key=api_key,
                temperature=0.1,
                max_tokens=4096,
            )
            logger.info(f"Initialized Claude LLM: {model}")

        elif "gpt" in model_lower:
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(
                model=model,
                openai_api_key=api_key,
                temperature=0.1,
                max_tokens=4096,
            )
            logger.info(f"Initialized OpenAI LLM: {model}")

        # Local LLMs
        elif "ollama" in model_lower:
            from langchain_ollama import ChatOllama

            # Extract model name (remove "ollama/" prefix if present)
            ollama_model = model.replace("ollama/", "")

            # Use provided base_url or default
            ollama_url = base_url or "http://localhost:11434"

            llm = ChatOllama(
                model=ollama_model,
                base_url=ollama_url,
                temperature=0.1,
                num_ctx=4096,  # Context window
            )
            logger.info(f"Initialized Ollama LLM: {ollama_model} at {ollama_url}")

        elif "vllm" in model_lower:
            # Option 1: Direct VLLM (for in-process inference)
            # from langchain_community.llms import VLLM
            # llm = VLLM(model=model_name, trust_remote_code=True, ...)

            # Option 2: vLLM via OpenAI-compatible API (recommended)
            from langchain_openai import ChatOpenAI

            # Extract model name
            vllm_model = model.replace("vllm/", "")
            vllm_url = base_url or "http://localhost:8000/v1"

            llm = ChatOpenAI(
                model=vllm_model,
                openai_api_key="EMPTY",  # vLLM doesn't need real key
                base_url=vllm_url,
                temperature=0.1,
                max_tokens=4096,
            )
            logger.info(f"Initialized vLLM: {vllm_model} at {vllm_url}")

        else:
            raise ValueError(
                f"Unsupported model: {model}. "
                f"Supported: claude-*, gpt-*, ollama/*, vllm/*"
            )

        # Create and return agent
        agent = MLPatrolAgent(llm=llm, verbose=verbose, **kwargs)
        return agent

    except ImportError as e:
        logger.error(f"Failed to import LLM library: {e}")
        raise ValueError(
            f"Required library not installed. "
            f"For Ollama: pip install langchain-ollama. "
            f"For Claude: pip install langchain-anthropic. "
            f"For OpenAI/vLLM: pip install langchain-openai"
        )
    except Exception as e:
        logger.error(f"Failed to create agent: {e}", exc_info=True)
        raise
```

#### 1.3 Update App Initialization

**File:** `app.py`

**Location:** Lines 110-140

```python
@classmethod
def initialize(cls) -> None:
    """Initialize the MLPatrol agent with API keys from environment."""
    try:
        logger.info("Initializing MLPatrol agent...")

        # Check for local LLM configuration first
        use_local = os.getenv("USE_LOCAL_LLM", "false").lower() == "true"

        if use_local:
            # Local LLM via Ollama
            model = os.getenv("LOCAL_LLM_MODEL", "ollama/llama3.1:8b")
            base_url = os.getenv("LOCAL_LLM_URL", "http://localhost:11434")

            logger.info(f"Using local LLM: {model}")
            cls._agent = create_mlpatrol_agent(
                model=model,
                base_url=base_url,
                verbose=True
            )
        else:
            # Cloud LLMs (existing logic)
            api_key = os.getenv("ANTHROPIC_API_KEY")
            model = "claude-sonnet-4-0"

            if not api_key:
                api_key = os.getenv("OPENAI_API_KEY")
                model = "gpt-4"

            if not api_key:
                cls._error = (
                    "No API key found. Please set ANTHROPIC_API_KEY or OPENAI_API_KEY "
                    "environment variable, or set USE_LOCAL_LLM=true for local models."
                )
                logger.error(cls._error)
                cls._initialized = True
                return

            cls._agent = create_mlpatrol_agent(
                api_key=api_key,
                model=model,
                verbose=True
            )

        logger.info(f"Agent initialized successfully with model: {model}")
        cls._initialized = True

    except Exception as e:
        cls._error = f"Failed to initialize agent: {str(e)}"
        logger.error(cls._error, exc_info=True)
        cls._initialized = True
```

#### 1.4 Update Environment Variables

**File:** `.env.example`

```bash
# MLPatrol Environment Variables

# ============================================================================
# LLM Configuration
# ============================================================================

# Option 1: Cloud LLMs (requires API key)
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Option 2: Local LLMs (no API key required)
USE_LOCAL_LLM=false  # Set to 'true' to use local LLM instead of cloud

# Local LLM Settings (only used if USE_LOCAL_LLM=true)
LOCAL_LLM_MODEL=ollama/llama3.1:8b  # Options: llama3.1:8b, llama3.1:70b, mistral-small:3.1
LOCAL_LLM_URL=http://localhost:11434  # Ollama default URL

# Optional: HuggingFace
HUGGINGFACE_API_KEY=your_hf_api_key_here

# Application Settings
LOG_LEVEL=INFO
MAX_AGENT_ITERATIONS=10
```

#### 1.5 Update Documentation

**File:** `README.md`

Add new section:

```markdown
## Local LLM Support

MLPatrol now supports running with local LLMs via Ollama - no cloud API required!

### Quick Start with Ollama

1. **Install Ollama**
   ```bash
   # Linux/Mac
   curl https://ollama.ai/install.sh | sh

   # Windows: Download from https://ollama.ai/download
   ```

2. **Pull a model**
   ```bash
   ollama pull llama3.1:8b  # Fast, recommended
   # or
   ollama pull llama3.1:70b  # Higher accuracy
   ```

3. **Configure MLPatrol**
   ```bash
   # In your .env file
   USE_LOCAL_LLM=true
   LOCAL_LLM_MODEL=ollama/llama3.1:8b
   ```

4. **Install dependencies**
   ```bash
   pip install langchain-ollama
   ```

5. **Run MLPatrol**
   ```bash
   python app.py
   ```

### Recommended Models

| Model | Size | RAM | Speed | Accuracy | Use Case |
|-------|------|-----|-------|----------|----------|
| llama3.1:8b | 8B | 8GB | Fast | Good | Daily use, quick analysis |
| llama3.1:70b | 70B | 48GB | Slow | Excellent | Complex security analysis |
| mistral-small:3.1 | 22B | 16GB | Medium | Very Good | Balanced performance |
| qwen2.5:14b | 14B | 12GB | Medium | Very Good | Code analysis |

### Benefits of Local LLMs

- ‚úÖ **Privacy:** All data stays on your machine
- ‚úÖ **No API costs:** No per-token charges
- ‚úÖ **No rate limits:** Use as much as you want
- ‚úÖ **Offline capable:** Works without internet
- ‚úÖ **Open source:** Full transparency

### Cloud vs Local

| Feature | Cloud (Claude/GPT) | Local (Ollama) |
|---------|-------------------|----------------|
| Setup | API key only | Install Ollama + model |
| Cost | Pay per token | Free (after hardware) |
| Privacy | Data sent to cloud | 100% local |
| Speed | Fast (varies) | Depends on hardware |
| Quality | Excellent | Good to Very Good |
| Internet | Required | Optional |
```

---

### Phase 2: Advanced Features (Optional)

#### 2.1 Model Switching UI

Add dropdown in Gradio interface to select model at runtime:

```python
with gr.Tab("‚öôÔ∏è Settings"):
    llm_provider = gr.Radio(
        choices=["Cloud (Claude)", "Cloud (GPT-4)", "Local (Ollama)"],
        value="Cloud (Claude)",
        label="LLM Provider"
    )

    ollama_model = gr.Dropdown(
        choices=["llama3.1:8b", "llama3.1:70b", "mistral-small:3.1"],
        value="llama3.1:8b",
        label="Ollama Model",
        visible=False
    )
```

#### 2.2 Automatic Model Download

```python
def ensure_ollama_model(model_name: str) -> bool:
    """Check if Ollama model exists, download if not."""
    try:
        import subprocess
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True
        )
        if model_name not in result.stdout:
            logger.info(f"Downloading Ollama model: {model_name}")
            subprocess.run(["ollama", "pull", model_name], check=True)
        return True
    except Exception as e:
        logger.error(f"Failed to ensure Ollama model: {e}")
        return False
```

#### 2.3 Performance Monitoring

```python
def get_llm_stats(llm_type: str) -> Dict[str, Any]:
    """Get performance stats for current LLM."""
    if llm_type == "ollama":
        # Check Ollama server stats
        try:
            response = requests.get("http://localhost:11434/api/tags")
            return response.json()
        except:
            return {}
    return {}
```

---

## Migration Guide for Users

### For Existing Cloud Users

**No changes required!** Cloud LLMs continue to work exactly as before.

### For New Local Users

1. Install Ollama
2. Pull a model (`ollama pull llama3.1:8b`)
3. Set `USE_LOCAL_LLM=true` in `.env`
4. Install `langchain-ollama`
5. Run MLPatrol

### Hybrid Setup

Users can switch between cloud and local:

```bash
# Use cloud for production analysis
USE_LOCAL_LLM=false
ANTHROPIC_API_KEY=sk-...

# Use local for testing/development
USE_LOCAL_LLM=true
LOCAL_LLM_MODEL=ollama/llama3.1:8b
```

---

## Testing Plan

### Unit Tests

```python
def test_ollama_agent_creation():
    """Test Ollama agent initialization."""
    agent = create_mlpatrol_agent(
        model="ollama/llama3.1:8b",
        base_url="http://localhost:11434"
    )
    assert agent is not None

def test_local_cve_search():
    """Test CVE search with local LLM."""
    agent = create_mlpatrol_agent(model="ollama/llama3.1:8b")
    result = agent.run("Check for numpy CVEs")
    assert result.answer is not None
```

### Integration Tests

1. **CVE Search:** Verify tool calling works with Ollama
2. **Dataset Analysis:** Test file upload and analysis
3. **Code Generation:** Verify code output quality
4. **Multi-turn Chat:** Test conversation context

### Performance Benchmarks

| Test | Claude Sonnet 4 | Ollama llama3.1:8b | Ollama llama3.1:70b |
|------|----------------|-------------------|---------------------|
| CVE Query | ~3s | ~5s | ~15s |
| Dataset Analysis | ~8s | ~12s | ~25s |
| Code Generation | ~5s | ~8s | ~18s |

---

## Security Considerations

### Benefits

‚úÖ **Data Privacy:** All queries and responses stay local
‚úÖ **No Third-Party Access:** No data sent to Anthropic/OpenAI
‚úÖ **Compliance:** Easier to meet regulatory requirements
‚úÖ **Audit Trail:** Complete control over logging

### Risks

‚ö†Ô∏è **Model Security:** Ensure models are from trusted sources
‚ö†Ô∏è **Local Network Exposure:** Ollama server runs on localhost
‚ö†Ô∏è **Model Poisoning:** Downloaded models could be compromised

### Mitigations

- Only download models from official Ollama registry
- Run Ollama on localhost (not exposed to network)
- Verify model checksums when possible
- Keep Ollama updated for security patches

---

## Cost Analysis

### Cloud LLM Costs (Current)

**Claude Sonnet 4:**
- Input: $3 per 1M tokens
- Output: $15 per 1M tokens

**Typical MLPatrol Usage:**
- CVE query: ~2,000 tokens input + 1,000 output = $0.021
- Dataset analysis: ~5,000 input + 2,000 output = $0.045
- 100 queries/month: **~$3-5/month**

### Local LLM Costs

**One-time:**
- GPU (if needed): $300-$2,000 (RTX 4060 Ti 16GB to RTX 4090)
- Or use existing hardware

**Ongoing:**
- Electricity: ~$5-10/month (depending on usage)
- No per-query costs

**Break-even:** ~6-12 months for moderate use

### For MLPatrol Users

Most users likely have sufficient hardware already:
- Apple M1/M2: Excellent for 8B-14B models
- Gaming PCs with 16GB+ VRAM: Good for 8B-70B models
- High-end workstations: Can run any model

---

## Alternatives Considered (Summary)

| Solution | Ease of Use | Performance | Integration | Recommendation |
|----------|-------------|-------------|-------------|----------------|
| **Ollama** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **RECOMMENDED** |
| vLLM | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | For production scale only |
| LM Studio | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Good for GUI users |
| LocalAI | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Enterprise alternative |

---

## Implementation Checklist

- [ ] Add `langchain-ollama` to requirements.txt
- [ ] Update `create_mlpatrol_agent()` with Ollama support
- [ ] Add local LLM detection in app initialization
- [ ] Update `.env.example` with local LLM variables
- [ ] Add Ollama setup to README.md
- [ ] Create Ollama quickstart guide
- [ ] Add unit tests for Ollama integration
- [ ] Test with llama3.1:8b model
- [ ] Test with mistral-small:3.1 model
- [ ] Verify tool calling works
- [ ] Update documentation
- [ ] Create migration guide
- [ ] Add troubleshooting section

---

## Next Steps

1. **Review this proposal** with team/stakeholders
2. **Prototype Ollama integration** (2-4 hours)
3. **Test with sample models** (4-8 hours)
4. **Update documentation** (2-4 hours)
5. **Release as optional feature** (beta)
6. **Gather user feedback**
7. **Iterate and improve**

---

## Conclusion

**Ollama is the clear choice for MLPatrol's local LLM integration:**

‚úÖ Minimal code changes (clean abstraction)
‚úÖ Excellent user experience (simple setup)
‚úÖ Perfect fit for use case (single-user security analysis)
‚úÖ Strong LangChain support (tool calling works)
‚úÖ Accessible hardware requirements (runs on laptops)
‚úÖ Privacy-focused (all local)
‚úÖ Cost-effective (no API fees)

**Implementation is straightforward:** Add Ollama support as a new model type in the existing factory function, maintain backward compatibility with cloud LLMs, and provide clear documentation for users.

---

**Prepared by:** Claude Code
**Status:** Ready for Review
**Estimated Implementation Time:** 8-16 hours
